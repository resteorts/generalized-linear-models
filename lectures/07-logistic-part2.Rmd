---
title: "Logistic Regression (Part II)"
author: "Rebecca C. Steorts some material from Chapter 6 of Roback and Legler text."
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
bibliography: references.bib
---

## Computing set up

```{r setup, echo = TRUE, message = FALSE, warning= FALSE}
library(tidyverse)
library(tidymodels)
library(knitr)
library(patchwork)
library(viridis)
library(kableExtra)
library(magrittr)
library(gridExtra)

knitr::opts_chunk$set(fig.width = 8,
                      fig.asp = 0.618, 
                      fig.retina = 3, 
                      dpt = 300, 
                      out.width = "90%",
                      fig.align = "center")

ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))

colors <- tibble::tibble(green = "#B5BA72")
```

## Announcements

1. Homework 5 due Thursday at 5 PM.
2. Quiz 3 released next week. 
3. Homework 6 just posted to github (corresponds with logistic regression on this module). 


## Topics

- Case Study of Reconstruction of Alabama

Notes based on Chapter 6 @roback2021beyond unless noted otherwise.


# Basics of logistic regression

## Bernoulli + Binomial random variables {.small}

Logistic regression is used to analyze data with two types of responses:


-   **Binary**: These responses take on two values success $(Y = 1)$ or failure $(Y = 0)$, yes $(Y = 1)$ or no $(Y = 0)$, etc.

$$P(Y = y) = p^y(1-p)^{1-y} \hspace{10mm} y = 0, 1$$

-   **Binomial**: Number of successes in a Bernoulli process, $n$ independent trials with a constant probability of success $p$.

$$P(Y = y) = {n \choose y}p^{y}(1-p)^{n - y} \hspace{10mm} y = 0, 1, \ldots, n$$

In both instances, the goal is to model $p$ the probability of success.


## Logistic regression model {.midi}

$$
\log\Big(\frac{p}{1-p}\Big) = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p
$$



-   The response variable, $\log\Big(\frac{p}{1-p}\Big)$, is the log(odds) of success, i.e. the logit



-   Use the model to calculate the probability of success $$\hat{p} = \frac{e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p}}{1 + e^{\beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p}}$$



-   When the response is a Bernoulli random variable, the probabilities can be used to classify each observation as a success or failure

## Logistic vs linear regression model

```{r, OLSlogistic, fig.align="center", out.width="80%", fig.cap='Graph from BMLR Chapter 6', echo=FALSE, warning=FALSE, message=FALSE}

set.seed(0)
dat <- tibble(x=runif(200, -5, 10),
                  p=exp(-2+1*x)/(1+exp(-2+1*x)),
                  y=rbinom(200, 1, p),
                  y2=.3408+.0901*x,
                  logit=log(p/(1-p)))
dat2 <- tibble(x = c(dat$x, dat$x),
               y = c(dat$y2, dat$p),
               `Regression model` = c(rep("linear", 200),
                                      rep("logistic", 200)))

ggplot() + 
  geom_point(data = dat, aes(x, y)) +
  geom_line(data = dat2, aes(x, y, linetype = `Regression model`, 
                             color = `Regression model`)) + 
  labs(title = "Linear vs. logistic regression models",
  subtitle = "for binary response data") + 
  scale_colour_manual(name = 'Regression model',
                      values = c('blue', 'red'), 
                      labels = c('linear', 'logistic'), guide ='legend')

#ggplot(dat, aes(x = x)) +
#  geom_point(aes(y = y)) +
#  geom_smooth(aes(y = y, color = "blue"), method = "lm", 
#              linetype = 1, se=FALSE) +
#  geom_line(aes(y = p, color = "red"), linetype = 2) +
#  scale_colour_manual(name = 'Regression model', 
#         values = c('blue', 'red'), 
#         labels = c('linear', 'logistic'), guide = 'legend')
```

## Logit link

Bernoulli and Binomial random variables can be written in one-parameter exponential family form, $f(y;\theta) = e^{[a(y)b(\theta) + c(\theta) + d(y)]}$

**Bernoulli**

$$f(y;p) = e^{y\log(\frac{p}{1-p}) + \log(1-p)}$$

**Binomial**

$$f(y;n,p) = e^{y\log(\frac{p}{1-p}) + n\log(1-p) + \log{n \choose y}}$$

. . .

They have the same canonical link $b(p) = \log\big(\frac{p}{1-p}\big)$

## Assumptions for logistic regression {.midi}

The following assumptions need to be satisfied to use logistic regression to make inferences

1. $\hspace{0.5mm}$ **Binary response**: The response is dichotomous (has two possible outcomes) or is the sum of dichotomous responses





2. $\hspace{0.5mm}$ **Independence**: The observations must be independent of one another



3. $\hspace{0.5mm}$ **Variance structure**: Variance of a binomial random variable is $np(1-p)$ $(n = 1 \text{ for Bernoulli})$ , so the variability is highest when $p = 0.5$



4. $\hspace{0.5mm}$ **Linearity**: The log of the odds ratio, $\log\big(\frac{p}{1-p}\big)$, must be a linear function of the predictors $x_1, \ldots, x_p$

## Case Study: Reconstructing Alabama

In a paper entitled "Reconstructing Alabama: Reconstruction Era Demographic and Statistical Research," Ben Bayer performed an analysis of data from 1870 to explain factors that influence voting on referendums related to railroad subsidies [@Bayer2011]. 

## Case Study: Reconstructing Alabama

Positive votes are thought to be inversely proportional to the distance a voter is from the proposed railroad. 

The racial composition of a community (as measured by the percentage of Black residents) is thought to be associated with voting behavior too. 

Goal: Was voting on railroad referenda related to distance from the proposed railroad line and the racial composition of a community (in Hale County)?

## Data Organization 

The unit of observation for this data is a community in Hale County. We will focus on the following variables from `RR_Data_Hale.csv` collected for each community. 

- `pctBlack` = the percentage of Black residents in the community

- `distance` = the distance, in miles, the proposed railroad is from the community

- `YesVotes` = the number of "Yes" votes in favor of the proposed railroad line (our primary response variable)

- `NumVotes` = total number of votes cast in the election

## Data Summary

```{r, include=FALSE, comment=NA}
community <- c("Carthage","Cederville","Greensboro","Havana")
pctBlack <- c("58.4","92.4","59.4","58.4")
distance <- c(17,7,0,12)
YesVotes <- c(61,0,1790,16)
NumVotes <- c(110,15,1804,68)
```

```{r, table2chp6, echo=FALSE, comment=NA}
#tab:sampleRows
table2chp6 <- data.frame(community, pctBlack, distance, YesVotes, NumVotes)
colnames(table2chp6) <- c("community","pctBlack", "distance","YesVotes","NumVotes")
kable(table2chp6, booktabs=T, 
      caption="Sample of the data for the Hale County, Alabama, railroad subsidy vote.") %>%
  kable_styling(full_width = F)
```

## EDA

```{r, include=FALSE}
rrHale.df = read.csv("data/RR_Data_Hale.csv")
rrHale.df <- rrHale.df %>%
  dplyr::select(community = County, pctBlack = pctBlack, 
                distance = distance, 
                YesVotes = YesVotes, 
                NumVotes = NumVotes) %>%
  filter(!is.na(pctBlack)) %>%
  mutate(propYes = YesVotes / NumVotes, 
         InFavor = (propYes > .50)) 
rrHale.df
```

```{r, echo = FALSE}
# checking correlation betwe predictors
with(rrHale.df,cor(pctBlack,distance))

# overall proportion Yes
sum(rrHale.df$YesVotes) / sum(rrHale.df$NumVotes)
# overall proportion Yes without Grennsboro
sum(rrHale.df$YesVotes[rrHale.df$community!="Greensboro"]) / 
  sum(rrHale.df$NumVotes[rrHale.df$community!="Greensboro"])
```

## Scatterplot


```{r, coded, fig.align="center",out.width="60%", fig.cap=' Scatterplot of distance from a proposed rail line and percent Black residents in the community coded by whether the community was in favor of the referendum or not.',echo=FALSE, warning=FALSE}
#Coded Scatterplot
ggplot(rrHale.df, aes(x=distance, y=pctBlack, color=InFavor)) + 
  geom_point(aes(shape = InFavor), size = 2.5) +
  #scale_shape_manual(values=c(1,2)) + # Use a slightly darker palette than normal
  ylab("Percent Black residents in the community") +
  xlab("Distance to the proposed railroad")
```

## Empirical Logit Plots

To assess the linearity assumption, we construct empirical logit plots, where this is based upon the sample. 

```{r,emplogits, fig.align="center",out.width="60%", fig.cap='Empirical logit plots for the Railroad Referendum data. The top plot is linear; the bottom plot reveals Greensboro deviates from the linear pattern.',echo=FALSE, warning=FALSE, message=FALSE}
#elogitXdist and elogitXperBlk
# To check the linearity assumption
# Look for outliers

## Empirical logit Plots
# Compute the empirical logits (added 0.5 to avoid log(0))
phat <- with(rrHale.df, (YesVotes+.5)/(NumVotes+1))
rrHale.df$elogit <- log(phat/(1-phat))
rrHale.df$Greensboro <- ifelse(rrHale.df$community=="Greensboro", "Greensboro", NA)
## Plots
logdis <- ggplot(rrHale.df, aes(x=distance, y=elogit))+
    geom_point(shape=1) +    # Use hollow circles
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE) + # Don't add shaded confidence region
  xlab("distance") + ylab("empirical logits") + 
  labs(title="Hale Empirical logits by distance")
logblack <- ggplot(rrHale.df, aes(x=pctBlack, y=elogit))+
    geom_point(shape=1) +    # Use hollow circles
    geom_smooth(method=lm,   # Add linear regression line
                se=FALSE) +   # Don't add shaded confidence region
  xlab("percent Black residents") + ylab("empirical logits") + 
  labs(title="Hale Empirical logits by percent Black residents") +
  geom_text(aes(label=Greensboro), nudge_x = 7.5, nudge_y = -.5)
grid.arrange(logdis, logblack ,ncol=1)
```

## Initial Models

The first model includes only one covariate, distance.

\footnotesize
```{r, comment = NA}
# Model with just distance
model.HaleD <- glm(cbind(YesVotes, NumVotes - YesVotes) ~
    distance, family = binomial, data = rrHale.df)
# alternative expression
model.HaleD.alt <- glm(YesVotes / NumVotes ~ distance, 
    weights = NumVotes, family = binomial, data = rrHale.df)
```

```{r, echo=FALSE, message=FALSE}
coef(summary(model.HaleD))
cat(" Residual deviance = ", 
    summary(model.HaleD)$deviance, " on ",
    summary(model.HaleD)$df.residual, "df", "\n",
    "Dispersion parameter = ", 
    summary(model.HaleD)$dispersion)
```

## Initial Model

Our estimated binomial regression model is:

\[\log\left(\frac{\hat{p}_i}{1-\hat{p}_i}\right)=3.309-0.288 \textrm{distance}_i\]
where $\hat{p}_i$ is the estimated proportion of Yes votes in community $i$.  

The estimated odds ratio for distance, that is the exponentiated coefficient for distance, in this model is $e^{-0.288}=0.750$. 

It can be interpreted as follows: for each additional mile from the proposed railroad, the support (odds of a Yes vote) declines by 25.0\%.

## Second Model

The covariate `pctBlack` is then added to the first model. 

\footnotesize
```{r, comment = NA}
model.HaleBD <- glm(cbind(YesVotes, NumVotes - YesVotes) ~
  distance + pctBlack, family = binomial, data = rrHale.df)
```

```{r, echo=FALSE, message=FALSE}
coef(summary(model.HaleBD))
cat(" Residual deviance = ", 
    summary(model.HaleBD)$deviance, " on ",
    summary(model.HaleBD)$df.residual, "df", "\n",
    "Dispersion parameter = ", 
    summary(model.HaleBD)$dispersion)
```

```{r, include=FALSE}
exp(coef(model.HaleBD))
```

## Second Model

Despite the somewhat strong negative correlation between percent Black residents and distance, the estimated odds ratio for distance remains approximately the same in this new model (OR $= e^{-0.29} = 0.747$).

That is, controlling for percent Black residents does little to change our estimate of the effect of distance. 

For each additional mile from the proposed railroad, odds of a Yes vote declines by 25.3\% after adjusting for the racial composition of a community.  

We also see that, for a fixed distance from the proposed railroad, the odds of a Yes vote declines by 1.3\% (OR $= e^{-.0132} = .987$) for each additional percent of Black residents in the community.

## Tests for Significance of Model Coefficients

Do we have statistically significant evidence that support for the railroad referendum decreases with higher proportions of Black residents in a community, after accounting for the distance a community is from the railroad line?  

We will investigate this using a drop in deviance test. 

Recall: the null is the reduced model and the alternative is the full model. 

## Tests for Significance of Model Coefficients

Our larger model is given by $\log\left(\frac{p_i}{1-p_i}\right) = \beta_0+\beta_1\textrm{distance}_i+\beta_2\textrm{pctBlack}_i.$

The drop-in-deviance test compares the larger model above to the reduced model $\log\left(\frac{p_i}{1-p_i}\right) = \beta_0+\beta_1\textrm{distance}_i$ by comparing residual deviances from the two models. 


## Tests for Significance of Model Coefficients

```{r comment=NA, message=FALSE}
drop_in_dev <- anova(model.HaleD, 
                     model.HaleBD, test = "Chisq")
```

```{r comment=NA, message=F, echo=F}
did_print <- data.frame(ResidDF=drop_in_dev$`Resid. Df`,
    ResidDev=drop_in_dev$`Resid. Dev`,
    Deviance=drop_in_dev$Deviance, Df=drop_in_dev$Df,
    pval=drop_in_dev$`Pr(>Chi)`)
row.names(did_print) <- row.names(drop_in_dev)
did_print
```

The drop-in-deviance test statistic is $318.44 - 307.22 = 11.22$ on $9 - 8 = 1$ df, producing a p-value of .00081. Thus, we reject the null in favor of the full model. 

We find that there is significant evidence that supports for the railroad referendum decreases with higher black residents in the community, after adjusting for the distance a community is from the railroad. 

## Confidence Intervals for Model Coefficients

We can use the  **profile likelihood method**, \index{profile likelihood} to find confidence intervals for our model coefficients. (Similar to Section 4.4).

```{r, comment=NA, message=FALSE}
exp(confint(model.HaleBD))
```

In the model with `distance` and `pctBlack`, the profile likelihood 95\% confidence interval for $e^{\beta_2}$ is (0.979, 0.994).  

Thus, we can be 95 percent confident that a 1 percent increase in the proportion of black residents is associated with a 0.6 percent to 2.1 percent decrease in the odds of a yes vote for the railroad, after controlling for distance. 

<!-- We can also confirm the statistically significant association between percent Black residents and odds of voting Yes (after controlling for distance), because 1 is not a plausible value of $e^{\beta_2}$ (where an odds ratio of 1 would imply that the odds of voting Yes do not change with percent Black residents). -->

## Confidence Intervals for Model Coefficients

We can alternatively calculate the confidence interval using the Wald statistic.

The 95$\%$ CI for $\beta_2$ is 
\begin{align}
\hat{\beta_2} &\pm 1.96 \times \text{SE}(\hat{\beta_2}) \\
& = -0.0132 \pm 1.96 \times 0.0039 \\
& = (-0.0208, -0.0056)
\end{align}

The 95$\%$ CI for $e^{\beta_2}$ is 
\begin{align}
& = (e^{-0.0208}, e^{-0.0056})\\
& = (0.979, 0.994),
\end{align}

matching the results for the profile likelihood. 

## Testing Goodness of Fit

We can conduct a goodness of fit test similar to Poisson models by comparing the residual deviance (307.22) to a $\chi^2$ distribution with $n-p$ degree of freedom (8).

```{r}
1 - pchisq(307.2173, 8)
```

The model with `pctBlack` and `distance` has statistically significant evidence of lack-of-fit ($p<.001$). 

## Testing Goodness of Fit

Similar to the Poisson regression models, this lack-of-fit could result from 

(a) missing covariates, 
(b) outliers, or 
(c) overdispersion.  

We will first attempt to address (a) by fitting a model with an interaction between distance and percent Black residents, to determine whether the effect of racial composition differs based on how far a community is from the proposed railroad. 

## Addressing Missing Covariates

\footnotesize
```{r, comment = NA}
model.HaleBxD <- glm(cbind(YesVotes, NumVotes - YesVotes) ~
  distance + pctBlack + distance:pctBlack, 
  family = binomial, data = rrHale.df)
```

```{r, echo=FALSE, message=FALSE}
coef(summary(model.HaleBxD))
cat(" Residual deviance = ", 
    summary(model.HaleBxD)$deviance, " on ",
    summary(model.HaleBxD)$df.residual, "df", "\n",
    "Dispersion parameter = ", 
    summary(model.HaleBxD)$dispersion)
```

## Drop in Deviance Test
```{r comment=NA, message=FALSE}
drop_in_dev <- anova(model.HaleBD, model.HaleBxD, 
                     test = "Chisq")
```

```{r comment=NA, message=F, echo=F}
did_print <- data.frame(ResidDF=drop_in_dev$`Resid. Df`,
    ResidDev=drop_in_dev$`Resid. Dev`,
    Deviance=drop_in_dev$Deviance, Df=drop_in_dev$Df,
    pval=drop_in_dev$`Pr(>Chi)`)
row.names(did_print) <- row.names(drop_in_dev)
did_print
```

## Drop in Deviance Test

We have statistically significant evidence (Drop-in-deviance test: $\chi^2=32.984, p<.001$) that the effect of the proportion of community residents who are Black on the odds of voting Yes depends on the distance of the community from the proposed railroad.  


## Interaction Model


  
Our interaction model still exhibits lack-of-fit (residual deviance of 274.23 on just 7 df).

A residual deviance of 274.23 is extremely large for a chi-squared distribution with 7 df. This corresponds to a p-value that is essentially zero — meaning the probability of observing such a large deviance by chance, if the model were correct, is virtually 0.

We will now assess the model potential outliers and overdispersion by examining the model's residuals.

## Residuals for Binomial Regression

There are two types of residuals used for Binomial regression:

1. the Pearson residual and
2. the deviance residual 

## Pearson residual

The Pearson residual is calculated using

$$\text{Pearson residual}_i = \frac{\text{actual count} - \text{predicted count}}{\text{SD of count}} 
= \frac{Y_i - m_i \hat{p}_i}{\sqrt{m_i\hat{p}_i(1 - \hat{p}_i)}}.$$

where $m_i$ is the number of trials for observation $i$ and $\hat{p}_i$ is the estimated probability of success for that same observation. 

## Deviance residual

A deviance residual is an alternative residual for binomial regression based on the discrepency between the observed values and those estimated using the likelihood. 

A deviance residual can be calculated for each observation using

$$d_i = \text{sign}(Y_i - m_i \hat{p}_i)
\sqrt{
2\left[
\log(
\frac{Y_i}
{m_i \hat{p}_i)}
)
+ 
(m_i - Y_i)
\log(
\frac{m_i - Y_i}
{m_i - m_i \hat{p}_i}
)
\right]
}
$$

When the number of trials is large for all observations and the models are appropriate, both sets of residuals should follow a standard normal distribution. 

## Residual Deviance 

The sum of the individual deviance residual is called the **deviance** or the **residual deviance**. 

A smaller deviance is preferred. 

For a good model fit, the deviance should follow $\chi^2_{n-p}$

## Residual Analysis

We consider a residual analysis of our interaction model by plotting the residuals against the fitted values. 

```{r, resid,fig.align="center",out.width="60%", fig.cap='Fitted values by residuals for the interaction model for the Railroad Referendum data.',echo=FALSE, warning=FALSE, comment=NA}
rrHale.df <- rrHale.df %>%
  mutate(resid.BxD = residuals(model.HaleBxD), 
         fit.BxD = fitted.values(model.HaleBxD))
ggplot(rrHale.df, aes(x = fit.BxD, y = resid.BxD)) +
  geom_point() +
  geom_text(aes(label=Greensboro), nudge_x = -.075) +
  xlab("Fitted values from interaction model") +
  ylab("Deviance residuals from interaction model")
```

## Residual Analysis

```{r, resid-duplicated,fig.align="center",out.width="60%", fig.cap='Fitted values by residuals for the interaction model for the Railroad Referendum data.',echo=FALSE, warning=FALSE, comment=NA}
rrHale.df <- rrHale.df %>%
  mutate(resid.BxD = residuals(model.HaleBxD), 
         fit.BxD = fitted.values(model.HaleBxD))
ggplot(rrHale.df, aes(x = fit.BxD, y = resid.BxD)) +
  geom_point() +
  geom_text(aes(label=Greensboro), nudge_x = -.075) +
  xlab("Fitted values from interaction model") +
  ylab("Deviance residuals from interaction model")
```

- Greensboro does not appear to be an outlier.
- It is possible that the binomial counts are overdispersed. 

## Overdispersion 

Similar to Poisson regression, we can adjust for overdispersion in binomial regression. 

There is extra-binomial variation, meaning the actual variance will be greater than the variance of the binomial variable $n p (1-p).$

## Adjustment for Overdispersion

To adjust for overdispersion, we can estimate a dispersion parameter $\hat{\phi}$ for the variance that will inflate it.

Specifically, 

$$\hat{\phi} = \frac{\sum (\text{Pearson residuals})^2}{n-p}$$

and this was the same approach taken in Section 4.9 for Poisson regression. 

## Adjustment for Overdispersion

When overdispersion is adjusted for in this manner, we cannot use MLE to fit our regression model. 

We must use a quasi-likelihood approach, which is similar to likelihood based inference, but because the model uses a dispersion parameter it is no longer a binomial model with a binomial likelihood. 

Thus, we call it quasi-binomial; R provides this for fitting the model. 

## Analysis for Overdispersion


```{r}
model.HaleBxDq <- glm(cbind(YesVotes, 
        NumVotes - YesVotes) ~
        distance + pctBlack + distance:pctBlack, 
        family = quasibinomial, data = rrHale.df)
```


## Analysis for Overdispersion

\footnotesize
```{r, echo=FALSE, message=FALSE}
coef(summary(model.HaleBxDq))
cat(" Residual deviance = ", 
    summary(model.HaleBxDq)$deviance, " on ",
    summary(model.HaleBxDq)$df.residual, "df", "\n",
    "Dispersion parameter = ", 
    summary(model.HaleBxDq)$dispersion)
```

Output adjusting the interaction model for overdispersion, where $\hat{\phi} = 51.6$ is used to adjust the standard errors for the coefficients and drop in deviance test. 

Standard errors are inflated by a factor of $\sqrt(51.6) = 7.2.$

Observe, there are no significant terms in the model below.

Thus, we remove the interaction terms and refit the model. 

## Removal of Interaction Term

```{r, comment = NA}
model.HaleBDq <- glm(cbind(YesVotes, 
       NumVotes - YesVotes) ~
       distance + pctBlack, 
       family = quasibinomial, data = rrHale.df)
```

## Removal of Interaction Term

\footnotesize 
```{r, echo=FALSE, message=FALSE}
coef(summary(model.HaleBDq))
cat(" Residual deviance = ", 
    summary(model.HaleBDq)$deviance, " on ",
    summary(model.HaleBDq)$df.residual, "df", "\n",
    "Dispersion parameter = ", 
    summary(model.HaleBDq)$dispersion)
```

By removing the interaction term and using the dispersion parameter, we see:

- distance is significantly associated with referendum support
- percent black is not significantly associated with referendum support (after adjusting for distance)

## Estimated coefficients

Quasi-likelihood methods do not change the estimated coefficients. 

We still estimate a 25 percent decline $(1 - e^{-0.292})$ in referdendum support for each additional mile from the proposed railroad. 

## Confidence intervals

```{r, comment=NA, message=FALSE}
exp(confint(model.HaleBDq))
```

Our previous $95\%$ confidence interval for the odds ratio associated with distance was $(0.728, 0.766).$

Our new $95\%$ confidence interval for the odds ratio associated with a distance is $(0.609, 0.891),$ which is **wider.**

## Summary

- We began fitting a logistic regression model with distance, solely. 
- We added pctBlack.
- We performed a drop in deviance test, providing strong support for the addition of pctBlack to the model. 
- The model with both distance and pctBlack had a large residual deviance suggesting an ill model fit. 

## Summary (continued)

- We investigated issues with lack of model fit. 
- Greensboro was perhaps an outlier, however, models with and without Greensboro were effectively the same.
- To account for the large deviance, we attempted to adjust for overdispersion.
- The final model included distance and pctBlack, although pctBlack
was no longer significant after adjust for dispersion. 

## References